{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yolo-web.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFaMWuFPBGqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2eb2536-8760-47e6-e2b8-95f2d7fdc5c8"
      },
      "source": [
        "import torch\n",
        "print('PyTorch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch 1.9.0+cu102 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15109MB, multi_processor_count=40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzM0SdckCm4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55505521-fa27-4aa3-f870-2ba312b136e3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "# import yolo project\n",
        "!rm -rf yolov3\n",
        "!git clone https://github.com/ultralytics/yolov3/\n",
        "\n",
        "# import training data project\n",
        "!rm -rf yolo-training-data\n",
        "!git clone https://github.com/bhecquet/yolo-training-data\n",
        "\n",
        "%cd yolov3"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content\n",
            "Cloning into 'yolov3'...\n",
            "remote: Enumerating objects: 9862, done.\u001b[K\n",
            "remote: Total 9862 (delta 0), reused 0 (delta 0), pack-reused 9862\u001b[K\n",
            "Receiving objects: 100% (9862/9862), 9.19 MiB | 26.44 MiB/s, done.\n",
            "Resolving deltas: 100% (6667/6667), done.\n",
            "Cloning into 'yolo-training-data'...\n",
            "remote: Enumerating objects: 9019, done.\u001b[K\n",
            "remote: Counting objects: 100% (9019/9019), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7349/7349), done.\u001b[K\n",
            "remote: Total 9019 (delta 1683), reused 8957 (delta 1637), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (9019/9019), 50.06 MiB | 31.51 MiB/s, done.\n",
            "Resolving deltas: 100% (1683/1683), done.\n",
            "/content/yolov3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bub53qzuoIcv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2856586e-652a-44c6-dd42-ab1fcb0519fa"
      },
      "source": [
        "# crÃ©ation des donnÃ©es de test / entrainement\n",
        "!python /content/yolo-training-data/generate_train_files.py /content/yolo-training-data/field-detector/dataset_extracted /content/yolo-training-data/field-detector/dataset_generated_small --output /content/yolov3/dataset"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:2721 files added to /content/yolov3/dataset/training\n",
            "INFO:root:290 files added to /content/yolov3/dataset/testing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBZWhwvlBUvF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "14856653-cf52-4114-d174-5099a0b304aa"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output \n",
        "print('PyTorch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n",
        "best_fitness = 0.0\n",
        "cfg = '/content/yolo-training-data/field-detector/data/web-generated.yaml'\n",
        "#cfg = 'web-generated.yaml'\n",
        "model = 'yolov3-tiny.pt'\n",
        "\n",
        "epoch_to_run = 3\n",
        "\n",
        "name = os.path.basename(cfg).split(\".\")[0] + '-' + model.split('.')[0]\n",
        "\n",
        "\n",
        "!python train.py --img 448 --batch 16 --epochs $epoch_to_run --data $cfg --weights $model --name $name --exist-ok \n",
        "weights_file_path = '/content/yolov3/runs/train/%s/weights/best.pt' % (name)\n",
        "w = torch.load(weights_file_path)\n",
        "\n",
        "fitness = w['best_fitness']\n",
        "print(\"epoch completed: \" + str(epoch_to_run))\n",
        "print(\"mAp: \" + str(fitness))\n",
        "\n",
        "!cp /content/yolov3/runs/train/{name}/weights/best.pt /content/drive/My\\ Drive/best_{name}_{epochs_completed}.pt\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch 1.9.0+cu102 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15109MB, multi_processor_count=40)\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov3/ âœ…\n",
            "YOLOv3 ðŸš€ v9.5.0-13-g1be3170 torch 1.9.0+cu102 CUDA:0 (Tesla T4, 15109.75MB)\n",
            "\n",
            "Namespace(adam=False, artifact_alias='latest', batch_size=16, bbox_interval=-1, bucket='', cache_images=False, cfg='', data='/content/yolo-training-data/field-detector/data/web-generated.yaml', device='', entity=None, epochs=3, evolve=False, exist_ok=True, global_rank=-1, hyp='data/hyp.scratch.yaml', image_weights=False, img_size=[448, 448], label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='web-generated-yolov3-tiny', noautoanchor=False, nosave=False, notest=False, project='runs/train', quad=False, rect=False, resume=False, save_dir='runs/train/web-generated-yolov3-tiny', save_period=-1, single_cls=False, sync_bn=False, total_batch_size=16, upload_dataset=False, weights='yolov3-tiny.pt', workers=8, world_size=1)\n",
            "\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "2021-07-29 09:33:29.598269: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0\n",
            "\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOv3 logging with 'pip install wandb' (recommended)\n",
            "Downloading https://github.com/ultralytics/yolov3/releases/download/v9.5.0/yolov3-tiny.pt to yolov3-tiny.pt...\n",
            "100% 16.9M/16.9M [00:00<00:00, 63.8MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=9\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1       464  models.common.Conv                      [3, 16, 3, 1]                 \n",
            "  1                -1  1         0  torch.nn.modules.pooling.MaxPool2d      [2, 2, 0]                     \n",
            "  2                -1  1      4672  models.common.Conv                      [16, 32, 3, 1]                \n",
            "  3                -1  1         0  torch.nn.modules.pooling.MaxPool2d      [2, 2, 0]                     \n",
            "  4                -1  1     18560  models.common.Conv                      [32, 64, 3, 1]                \n",
            "  5                -1  1         0  torch.nn.modules.pooling.MaxPool2d      [2, 2, 0]                     \n",
            "  6                -1  1     73984  models.common.Conv                      [64, 128, 3, 1]               \n",
            "  7                -1  1         0  torch.nn.modules.pooling.MaxPool2d      [2, 2, 0]                     \n",
            "  8                -1  1    295424  models.common.Conv                      [128, 256, 3, 1]              \n",
            "  9                -1  1         0  torch.nn.modules.pooling.MaxPool2d      [2, 2, 0]                     \n",
            " 10                -1  1   1180672  models.common.Conv                      [256, 512, 3, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.padding.ZeroPad2d      [[0, 1, 0, 1]]                \n",
            " 12                -1  1         0  torch.nn.modules.pooling.MaxPool2d      [2, 1, 0]                     \n",
            " 13                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 1]             \n",
            " 14                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 15                -1  1   1180672  models.common.Conv                      [256, 512, 3, 1]              \n",
            " 16                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 18           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
            " 19                -1  1    885248  models.common.Conv                      [384, 256, 3, 1]              \n",
            " 20          [19, 15]  1     32340  models.yolo.Detect                      [9, [[10, 14, 23, 27, 37, 58], [81, 82, 135, 169, 344, 319]], [256, 512]]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "Model Summary: 59 layers, 8688356 parameters, 8688356 gradients, 13.0 GFLOPS\n",
            "\n",
            "Transferred 68/72 items from yolov3-tiny.pt\n",
            "\n",
            "WARNING: Dataset not found, nonexistent paths: ['/content/yolov3/yolo-training-data/dataset/testing/images']\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 541, in <module>\n",
            "    train(hyp, opt, device, tb_writer)\n",
            "  File \"train.py\", line 97, in train\n",
            "    check_dataset(data_dict)  # check\n",
            "  File \"/content/yolov3/utils/general.py\", line 213, in check_dataset\n",
            "    raise Exception('Dataset not found.')\n",
            "Exception: Dataset not found.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-82e4cb6e49a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python train.py --img 448 --batch 16 --epochs $epoch_to_run --data $cfg --weights $model --name $name --exist-ok '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mweights_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/yolov3/runs/train/%s/weights/best.pt'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mfitness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_fitness'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/yolov3/runs/train/web-generated-yolov3-tiny/weights/best.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w0zxEoVfGVj"
      },
      "source": [
        "from utils import utils; utils.plot_results()\n",
        "\n",
        "w = torch.load('weights/best.pt')\n",
        "epochs_completed = str(w['epoch'])\n",
        "fitness = w['best_fitness']\n",
        "print(\"epoch completed: \" + epochs_completed)\n",
        "print(\"mAp: \" + str(fitness))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-kDXOW3bQVL",
        "outputId": "ddeb899e-1a0e-487f-82fd-2134259e1437"
      },
      "source": [
        "!python3 detect.py  --source \"dataset_real/ac3bfc152e8ba5ba0b3423755fe6d234.jpg\" --weights /content/drive/My\\ Drive/best_web-generated_-1.pt --img-size 640 --exist-ok  --save-txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(agnostic_nms=False, augment=False, classes=None, conf_thres=0.25, device='', exist_ok=True, img_size=640, iou_thres=0.45, name='exp', nosave=False, project='runs/detect', save_conf=False, save_txt=True, source='dataset_real/ac3bfc152e8ba5ba0b3423755fe6d234.jpg', update=False, view_img=False, weights=['/content/drive/My Drive/best_web-generated_-1.pt'])\n",
            "YOLOv3 ðŸš€ bh1-8-g0ae89779 torch 1.8.1+cu101 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 269 layers, 62589598 parameters, 0 gradients, 155.9 GFLOPS\n",
            "image 1/1 /content/yolov3/dataset_real/ac3bfc152e8ba5ba0b3423755fe6d234.jpg: 384x640 8 fields, 2 radios, 3 buttons, 2 radio_with_labels, Done. (0.026s)\n",
            "Results saved to runs/detect/exp\n",
            "10 labels saved to runs/detect/exp/labels\n",
            "Done. (0.066s)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}